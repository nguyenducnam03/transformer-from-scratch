# Transformer From Scratch

## Overview

**Transformer From Scratch** is a Python implementation of the Transformer model architecture, built entirely from scratch for educational purposes. The project aims to provide a deeper understanding of how the Transformer model works by implementing all components of the architecture using pure Python and object-oriented programming (OOP). 

The model covers key components such as:
- **Multi-Head Attention**: Implementing attention mechanisms to capture long-range dependencies.
- **Positional Encoding**: Encoding the order of the input sequence for attention.
- **Feed-Forward Networks**: Implementing fully connected layers.
- **Encoder-Decoder Architecture**: Building the full model from both encoder and decoder stacks.
  
This project serves as a great learning tool for anyone looking to understand the internal workings of the Transformer model, from attention layers to final output generation.

## Features

- **Multi-Class Architecture**: A modular design using Python classes for each Transformer component, including Attention Mechanism, Positional Encoding, and Feed-Forward Networks.
- **Pure Python**: No external deep learning libraries (like TensorFlow or PyTorch) are used. All operations are implemented using core Python and NumPy.
- **Study-Oriented**: Step-by-step construction of the Transformer model, perfect for learning or teaching how Transformers function.

## Setup and Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/transformer-from-scratch.git
   cd transformer-from-scratch
1. **Install dependencies:**:
   ```bash
   pip install -r requirements.txt
